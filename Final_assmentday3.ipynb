{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bafa729-0aaf-4ea0-becf-2d1a6e7d6b35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_daily_hal = spark.read.csv(\"/mnt/vatsal-test/Final/daily_HAL.BSE.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_daily_infy = spark.read.csv(\"/mnt/vatsal-test/Final/daily_INFY.BSE.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_daily_tcs = spark.read.csv(\"/mnt/vatsal-test/Final/daily_TCS.BSE.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_monthly_hal = spark.read.csv(\"/mnt/vatsal-test/Final/monthly_HAL.BSE.csv\", header=True, inferSchema=True)\n",
    "df_monthly_infy = spark.read.csv(\"/mnt/vatsal-test/Final/monthly_INFY.BSE.csv\", header=True, inferSchema=True)\n",
    "df_monthly_tcs = spark.read.csv(\"/mnt/vatsal-test/Final/monthly_TCS.BSE.csv\", header=True, inferSchema=True)\n",
    "df_weekly_hal =spark.read.csv(\"/mnt/vatsal-test/Final/weekly_HAL.BSE.csv\", header=True, inferSchema=True)\n",
    "df_weekly_infy = spark.read.csv(\"/mnt/vatsal-test/Final/weekly_INFY.BSE.csv\", header=True, inferSchema=True)\n",
    "df_weekly_tcs = spark.read.csv(\"/mnt/vatsal-test/Final/weekly_TCS.BSE.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4913e4-a64b-476b-80cd-10d8bc4abf43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "def clean_data(df):\n",
    "    return df.withColumn(\"open\", col(\"open\").cast(\"float\")) \\\n",
    "             .withColumn(\"high\", col(\"high\").cast(\"float\")) \\\n",
    "             .withColumn(\"low\", col(\"low\").cast(\"float\")) \\\n",
    "             .withColumn(\"close\", col(\"close\").cast(\"float\")) \\\n",
    "             .withColumn(\"volume\", col(\"volume\").cast(\"long\")) \\\n",
    "             .dropna()\n",
    "\n",
    "df_daily_hal = clean_data(df_daily_hal)\n",
    "df_daily_infy= clean_data(df_daily_infy)\n",
    "df_daily_tcs= clean_data(df_daily_tcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0785d819-0723-494f-ab6e-964aa00aad81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    return df.withColumn(\"open\", col(\"open\").cast(\"float\")) \\\n",
    "             .withColumn(\"high\", col(\"high\").cast(\"float\")) \\\n",
    "             .withColumn(\"low\", col(\"low\").cast(\"float\")) \\\n",
    "             .withColumn(\"close\", col(\"close\").cast(\"float\")) \\\n",
    "             .withColumn(\"volume\", col(\"volume\").cast(\"long\")) \\\n",
    "             .dropna()\n",
    "\n",
    "df_monthly_hal= clean_data(df_monthly_hal)\n",
    "df_monthly_infy= clean_data(df_monthly_infy)\n",
    "df_monthly_tcs= clean_data(df_monthly_tcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42607c6f-335e-4d68-9c2d-f7148730b5ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    return df.withColumn(\"open\", col(\"open\").cast(\"float\")) \\\n",
    "             .withColumn(\"high\", col(\"high\").cast(\"float\")) \\\n",
    "             .withColumn(\"low\", col(\"low\").cast(\"float\")) \\\n",
    "             .withColumn(\"close\", col(\"close\").cast(\"float\")) \\\n",
    "             .withColumn(\"volume\", col(\"volume\").cast(\"long\")) \\\n",
    "             .dropna()\n",
    "df_weekly_hal= clean_data(df_weekly_hal)\n",
    "df_weekly_infy= clean_data(df_weekly_infy)\n",
    "df_weekly_tcs= clean_data(df_weekly_tcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125f3e34-e93b-4f60-a804-9db65dac96af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def add_symbol_column(df, symbol):\n",
    "    return df.withColumn(\"symbol\", lit(symbol))\n",
    "\n",
    "df_weekly_hal = add_symbol_column(df_weekly_hal, \"HAL\")\n",
    "df_weekly_infy = add_symbol_column(df_weekly_infy, \"INFY\")\n",
    "df_weekly_tcs = add_symbol_column(df_weekly_tcs, \"TCS\")\n",
    "\n",
    "df_monthly_hal = add_symbol_column(df_monthly_hal, \"HAL\")\n",
    "df_monthly_infy = add_symbol_column(df_monthly_infy, \"INFY\")\n",
    "df_monthly_tcs = add_symbol_column(df_monthly_tcs, \"TCS\")\n",
    "\n",
    "df_daily_hal = add_symbol_column(df_daily_hal, \"HAL\")\n",
    "df_daily_infy = add_symbol_column(df_daily_infy, \"INFY\")\n",
    "df_daily_tcs = add_symbol_column(df_daily_tcs, \"TCS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6dee9bc-07c4-4d9a-b4ae-14c6b6f2eb73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f_daily_weekly_monthly_hal = df_daily_hal.join(df_weekly_hal, on=[\"symbol\", \"timestamp\"], how=\"inner\") \\\n",
    "                                          .join(df_monthly_hal, on=[\"symbol\", \"timestamp\"], how=\"inner\") \\\n",
    "                                          .select(\n",
    "                                              df_daily_hal[\"symbol\"],\n",
    "                                              df_daily_hal[\"timestamp\"],\n",
    "                                              df_daily_hal[\"open\"].alias(\"daily_open\"),\n",
    "                                              df_daily_hal[\"high\"].alias(\"daily_high\"),\n",
    "                                              df_daily_hal[\"low\"].alias(\"daily_low\"),\n",
    "                                              df_weekly_hal[\"open\"].alias(\"weekly_open\"),\n",
    "                                              df_weekly_hal[\"high\"].alias(\"weekly_high\"),\n",
    "                                              df_weekly_hal[\"low\"].alias(\"weekly_low\")\n",
    "                                          )\n",
    "\n",
    "df_daily_weekly_monthly_infy = df_daily_infy.join(df_weekly_infy, on=[\"symbol\", \"timestamp\"], how=\"inner\") \\\n",
    "                                            .join(df_monthly_infy, on=[\"symbol\", \"timestamp\"], how=\"inner\") \\\n",
    "                                            .select(\n",
    "                                                df_daily_infy[\"symbol\"],\n",
    "                                                df_daily_infy[\"timestamp\"],\n",
    "                                                df_daily_infy[\"open\"].alias(\"daily_open\"),\n",
    "                                                df_daily_infy[\"high\"].alias(\"daily_high\"),\n",
    "                                                df_daily_infy[\"low\"].alias(\"daily_low\"),\n",
    "                                                df_weekly_infy[\"open\"].alias(\"weekly_open\"),\n",
    "                                                df_weekly_infy[\"high\"].alias(\"weekly_high\"),\n",
    "                                                df_weekly_infy[\"low\"].alias(\"weekly_low\")\n",
    "                                            )\n",
    "\n",
    "df_daily_weekly_monthly_tcs = df_daily_tcs.join(df_weekly_tcs, on=[\"symbol\", \"timestamp\"], how=\"inner\") \\\n",
    "                                          .join(df_monthly_tcs, on=[\"symbol\", \"timestamp\"], how=\"inner\") \\\n",
    "                                          .select(\n",
    "                                              df_daily_tcs[\"symbol\"],\n",
    "                                              df_daily_tcs[\"timestamp\"],\n",
    "                                              df_daily_tcs[\"open\"].alias(\"daily_open\"),\n",
    "                                              df_daily_tcs[\"high\"].alias(\"daily_high\"),\n",
    "                                              df_daily_tcs[\"low\"].alias(\"daily_low\"),\n",
    "                                              df_weekly_tcs[\"open\"].alias(\"weekly_open\"),\n",
    "                                              df_weekly_tcs[\"high\"].alias(\"weekly_high\"),\n",
    "                                              df_weekly_tcs[\"low\"].alias(\"weekly_low\")\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abff982-8d78-4413-91f6-a25cc99ff208",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col, avg, to_date, weekofyear, month\n",
    "\n",
    "def enrich_with_kpis(df):\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(\"timestamp\")\n",
    "    df = df.withColumn(\"daily_price_change_pct\", ((col(\"close\") - col(\"open\")) / col(\"open\") * 100)) \\\n",
    "           .withColumn(\"volume_change\", col(\"volume\") - lag(\"volume\").over(window_spec))\n",
    "    return df\n",
    "df_daily_hal = enrich_with_kpis(df_daily_hal)\n",
    "df_daily_infy = enrich_with_kpis(df_daily_infy)\n",
    "df_daily_tcs = enrich_with_kpis(df_daily_tcs)\n",
    "\n",
    "#aggregate KPIs over daily, weekly, and monthly periods\n",
    "def aggregate_kpis(df, period_col):\n",
    "    return df.groupBy(\"symbol\", period_col).agg(\n",
    "        avg(\"daily_price_change_pct\").alias(f\"avg_{period_col}_price_change_pct\"),\n",
    "        avg(\"volume_change\").alias(f\"avg_{period_col}_volume_change\")\n",
    "    )\n",
    "df_daily_hal = df_daily_hal.withColumn(\"day\", to_date(col(\"timestamp\")))\n",
    "df_daily_hal = df_daily_hal.withColumn(\"week\", weekofyear(col(\"timestamp\")))\n",
    "df_daily_hal = df_daily_hal.withColumn(\"month\", month(col(\"timestamp\")))\n",
    "\n",
    "df_daily_infy = df_daily_infy.withColumn(\"day\", to_date(col(\"timestamp\")))\n",
    "df_daily_infy = df_daily_infy.withColumn(\"week\", weekofyear(col(\"timestamp\")))\n",
    "df_daily_infy = df_daily_infy.withColumn(\"month\", month(col(\"timestamp\")))\n",
    "\n",
    "df_daily_tcs = df_daily_tcs.withColumn(\"day\", to_date(col(\"timestamp\")))\n",
    "df_daily_tcs = df_daily_tcs.withColumn(\"week\", weekofyear(col(\"timestamp\")))\n",
    "df_daily_tcs = df_daily_tcs.withColumn(\"month\", month(col(\"timestamp\")))\n",
    "\n",
    "# Aggregate KPIs over daily, weekly, and monthly periods\n",
    "df_daily_aggregated_hal = aggregate_kpis(df_daily_hal, \"day\")\n",
    "df_weekly_aggregated_hal = aggregate_kpis(df_daily_hal, \"week\")\n",
    "df_monthly_aggregated_hal = aggregate_kpis(df_daily_hal, \"month\")\n",
    "\n",
    "df_daily_aggregated_infy = aggregate_kpis(df_daily_infy, \"day\")\n",
    "df_weekly_aggregated_infy = aggregate_kpis(df_daily_infy, \"week\")\n",
    "df_monthly_aggregated_infy = aggregate_kpis(df_daily_infy, \"month\")\n",
    "\n",
    "df_daily_aggregated_tcs = aggregate_kpis(df_daily_tcs, \"day\")\n",
    "df_weekly_aggregated_tcs = aggregate_kpis(df_daily_tcs, \"week\")\n",
    "df_monthly_aggregated_tcs = aggregate_kpis(df_daily_tcs, \"month\")\n",
    "\n",
    "# df_daily_aggregated_hal.show()\n",
    "# df_weekly_aggregated_hal.show()\n",
    "# df_monthly_aggregated_hal.show()\n",
    "\n",
    "# df_daily_aggregated_infy.show()\n",
    "# df_weekly_aggregated_infy.show()\n",
    "# df_monthly_aggregated_infy.show()\n",
    "\n",
    "# df_daily_aggregated_tcs.show()\n",
    "# df_weekly_aggregated_tcs.show()\n",
    "# df_monthly_aggregated_tcs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cfbfcde-96b5-4bc3-9054-abc54543b579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_daily_aggregated_hal.write.csv(\"/mnt/vatsal-test/Final/Output_daily_hal_agggg.csv\", header=True)\n",
    "# df_weekly_aggregated_hal.write.csv(\"/mnt/vatsal-test/Final/Output_weekly_hal_agg.csv\", header=True)\n",
    "# df_monthly_aggregated_hal.write.csv(\"/mnt/vatsal-test/Final/Output_monthly_hal_agg.csv\")\n",
    "\n",
    "# df_daily_aggregated_infy.write.csv(\"/mnt/vatsal-test/Final/Output_daily_infy_agg.\")\n",
    "# df_weekly_aggregated_infy.write.csv(\"/mnt/vatsal-test/Final/Output_weekly_infy_agg.\")\n",
    "# df_monthly_aggregated_infy.write.csv(\"/mnt/vatsal-test/Final/Output_monthly_infy_agg\")\n",
    "\n",
    "# df_daily_aggregated_tcs.write.csv(\"/mnt/vatsal-test/Final/Output_daily_tcs_agg.csv\")\n",
    "# df_weekly_aggregated_tcs.write.csv(\"/mnt/vatsal-test/Final/Output_weekly_tcs_agg.csv\")\n",
    "# df_monthly_aggregated_tcs.write.csv(\"/mnt/vatsal-test/Final/Output_monthly_tcs_agg.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc46b2e-e61d-4c63-abd2-5df703613b83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_assmentday3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
